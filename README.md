# Data Engineering Portfolio

---

## **Technical Skills**
- **Current**: Python (NumPy, Pandas, Matplotlib, PySpark), SQL (MySQL), AWS (S3, SageMaker) 
- **In Progress**: UPDATE

---

## **Education**
**B.S. in Statistics** | Minor in Data Science  
California Polytechnic State University, San Luis Obispo (_June 2023_)

---

## **Portfolio Objective**
This portfolio showcases end-to-end data engineering capabilities, covering ETL, database design, batch/stream processing, and data analytics. Each project emphasizes practical experience with datasets and tools relevant to modern data workflows.

---

## **Datasets**
### Public Safety and Law Enforcement Dataset (LAPD)
- **Details**: Historical crime data from the LAPD, including incident type, location, timestamp, and resolution status.  
- **Use Cases**: Database management, data warehousing, and dashboard visualization.  
- **Link**: [LAPD Public Safety and Law Enforcement Dataset](https://www.kaggle.com/datasets/cityofLA/los-angeles-public-safety-and-law-enforcement)

### Smart-City CCTV Dataset (Public Safety)
- **Details**: Structured and unstructured data from traffic and safety monitoring systems, including timestamps, vehicle counts, and anomaly events.  
- **Use Cases**: Streaming data processing, batch aggregations, and predictive modeling.
- **Link**: [Smart-City CCTV Violence Detection Dataset (SCVD)](https://www.kaggle.com/datasets/toluwaniaremu/smartcity-cctv-violence-detection-dataset-scvd/data)

---

## **Projects**

### **Phase 1: Foundational Projects**

#### **Project 1: Data Pipeline for Ingesting Public Safety Data**
- **Skills**: ETL, Python (Pandas, Requests), PostgreSQL, Airflow 
- **Description**: Develop an ETL pipeline to ingest LAPD data into a PostgreSQL database. Leverage Airflow to automate data extraction, transformation, and loading, ensuring data quality and consistency. 
- **Programs Used**: Jupyter Notebook, PostgreSQL, Airflow

#### **Project 2: Database Schema Design and SQL Optimization**
- **Skills**: Database schema design, SQL indexing, performance tuning  
- **Description**: Create an optimized relational schema for the LAPD dataset. Focus on improving query performance for analysis and reporting needs.  
- **Programs Used**: 

#### **Project 3: Data Lake for Storage of Unstructured Data**
- **Skills**: AWS S3, Data Lake Architecture, Unstructured Data Management  
- **Description**: Set up a data lake on AWS S3 to store structured and unstructured crime data from the LAPD dataset. Enable scalable storage for future analysis.  
- **Programs Used**: 

#### **Project 4: Automated Data Cleaning and Transformation Pipeline**
- **Skills**: Python (Pandas), Data Transformation, Automation  
- **Description**: Design a Python-based pipeline to clean and preprocess raw crime data. Address missing values, inconsistent formats, and prepare data for analysis.  
- **Programs Used**: 

---

### **Phase 2: Intermediate Projects**

#### **Project 5: Data Warehousing with Star Schema for Crime Analysis**
- **Skills**: Star Schema Design, OLAP, SQL Query Optimization  
- **Description**: Develop a data warehouse using a star schema. Analyze trends in crime data by time, location, and type using LAPD dataset as the fact table.  
- **Programs Used**: 

#### **Project 6: Interactive Dashboard Using Tableau and Power BI**
- **Skills**: Data Visualization, Dashboard Development  
- **Description**: Create dynamic dashboards to visualize key trends in crime data, enabling stakeholders to interact with the data.  
- **Programs Used**: 

#### **Project 7: API Development for Public Safety Data Access**
- **Skills**: Flask/Django, API Development, JSON Data Handling  
- **Description**: Build a RESTful API for accessing processed LAPD dataset. Provide endpoints for querying incident details and generating summaries.  
- **Programs Used**: 

---

### **Phase 3: Advanced Projects**

#### **Project 8: Streaming Data with Kafka**
- **Skills**: Real-Time Data Processing, Kafka Topics, Partitioning  
- **Description**: Use Kafka to process real-time event data from the Smart-City CCTV dataset. Analyze traffic patterns and detect anomalies in real-time.  
- **Programs Used**: 

#### **Project 9: Batch Processing and Aggregation with Apache Spark**
- **Skills**: PySpark, Distributed Data Processing, Batch Aggregation  
- **Description**: Apply Apache Spark to process large batches of CCTV data. Generate insights such as average traffic flow and peak anomaly times.  
- **Programs Used**: 

#### **Project 10: Predictive Modeling for Crime Forecasting**
- **Skills**: Machine Learning, AWS SageMaker, Feature Engineering  
- **Description**: Train and validate predictive models to forecast crime trends using historical CCTV data. Use AWS SageMaker to streamline the ML workflow.  
- **Programs Used**: 


# Personal Project Portfolio Status

| **Project Name**                              | **Phase**             | **Status**         |
|-----------------------------------------------|-----------------------|--------------------|
| Data Pipeline for Ingesting Public Safety Data| Foundational (Phase 1)| In Progress        |
| Database Schema Design and SQL Optimization   | Foundational (Phase 1)| Not Started        |
| Data Lake for Storage of Unstructured Data    | Foundational (Phase 1)| Not Started        |
| Automated Data Cleaning and Transformation    | Foundational (Phase 1)| Not Started        |
| Data Warehousing with Star Schema             | Intermediate (Phase 2)| Not Started        |
| Interactive Dashboard Using Tableau           | Intermediate (Phase 2)| Not Started        |
| API Development for Public Safety Data Access | Intermediate (Phase 2)| Not Started        |
| Streaming Data with Kafka                     | Advanced (Phase 3)    | Not Started        |
| Batch Processing and Aggregation with Spark   | Advanced (Phase 3)    | Not Started        |
| Predictive Modeling for Crime Forecasting     | Advanced (Phase 3)    | Not Started        |
